{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building Transformer Blocks\n",
        "\n",
        "This notebook provides an interactive guide to understanding this component of GPT.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add project root to path\n",
        "project_root = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "# Import our transformer block components\n",
        "from src.model.blocks import LayerNorm, GELU, FeedForward\n",
        "from src.model.attention import MultiHeadAttention\n",
        "from src.model.gpt import TransformerBlock\n",
        "from src.config import GPTConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Transformer Blocks\n",
        "\n",
        "This notebook explores the building blocks of a transformer: Layer Normalization, GELU activation, Feed-Forward networks, and how they combine into a Transformer Block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Layer Normalization\n",
        "\n",
        "Layer normalization stabilizes training by normalizing inputs across the feature dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a LayerNorm module\n",
        "embedding_dim = 64\n",
        "layer_norm = LayerNorm(embedding_dim)\n",
        "\n",
        "# Test with random input\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "x = torch.randn(batch_size, seq_len, embedding_dim)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Input mean (per sequence): {x.mean(dim=-1)[0, :5]}\")\n",
        "print(f\"Input std (per sequence): {x.std(dim=-1)[0, :5]}\")\n",
        "\n",
        "# Apply layer norm\n",
        "normalized = layer_norm(x)\n",
        "\n",
        "print(f\"\\nAfter LayerNorm:\")\n",
        "print(f\"Output shape: {normalized.shape}\")\n",
        "print(f\"Output mean (per sequence): {normalized.mean(dim=-1)[0, :5]}\")\n",
        "print(f\"Output std (per sequence): {normalized.std(dim=-1)[0, :5]}\")\n",
        "\n",
        "# Verify normalization (should be close to 0 mean, 1 std)\n",
        "print(f\"\\nVerification:\")\n",
        "print(f\"Mean close to 0: {normalized.mean(dim=-1).abs().mean().item():.6f}\")\n",
        "print(f\"Std close to 1: {normalized.std(dim=-1).mean().item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the effect of layer normalization\n",
        "x = torch.randn(1, 20, embedding_dim)\n",
        "normalized = layer_norm(x)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Before normalization\n",
        "axes[0].plot(x[0, :, 0].detach().numpy(), label='Feature 0', alpha=0.7)\n",
        "axes[0].plot(x[0, :, 1].detach().numpy(), label='Feature 1', alpha=0.7)\n",
        "axes[0].set_title('Before LayerNorm')\n",
        "axes[0].set_xlabel('Sequence Position')\n",
        "axes[0].set_ylabel('Value')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# After normalization\n",
        "axes[1].plot(normalized[0, :, 0].detach().numpy(), label='Feature 0', alpha=0.7)\n",
        "axes[1].plot(normalized[0, :, 1].detach().numpy(), label='Feature 1', alpha=0.7)\n",
        "axes[1].set_title('After LayerNorm')\n",
        "axes[1].set_xlabel('Sequence Position')\n",
        "axes[1].set_ylabel('Value')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. GELU Activation Function\n",
        "\n",
        "GELU (Gaussian Error Linear Unit) is a smooth activation function used in GPT models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create GELU activation\n",
        "gelu = GELU()\n",
        "\n",
        "# Compare GELU with ReLU\n",
        "x = torch.linspace(-4, 4, 100)\n",
        "gelu_output = gelu(x)\n",
        "relu_output = torch.relu(x)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x.numpy(), gelu_output.numpy(), label='GELU', linewidth=2)\n",
        "plt.plot(x.numpy(), relu_output.numpy(), label='ReLU', linewidth=2, linestyle='--')\n",
        "plt.xlabel('Input', fontsize=12)\n",
        "plt.ylabel('Output', fontsize=12)\n",
        "plt.title('GELU vs ReLU Activation Functions', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axhline(0, color='black', linewidth=0.5)\n",
        "plt.axvline(0, color='black', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key differences:\")\n",
        "print(\"- GELU is smooth (differentiable everywhere)\")\n",
        "print(\"- GELU allows small negative values through\")\n",
        "print(\"- GELU is used in GPT models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Feed-Forward Network\n",
        "\n",
        "The feed-forward network applies two linear transformations with GELU activation in between."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a FeedForward network\n",
        "embedding_dim = 64\n",
        "ffn = FeedForward(embedding_dim)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in ffn.parameters())\n",
        "print(f\"FeedForward network:\")\n",
        "print(f\"  Embedding dimension: {embedding_dim}\")\n",
        "print(f\"  Hidden dimension: {4 * embedding_dim} (4x expansion)\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "\n",
        "# Test forward pass\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "x = torch.randn(batch_size, seq_len, embedding_dim)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "output = ffn(x)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Shape preserved: {x.shape == output.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the feed-forward transformation\n",
        "x = torch.randn(1, 1, embedding_dim)\n",
        "output = ffn(x)\n",
        "\n",
        "# Get intermediate activations by accessing the network layers\n",
        "with torch.no_grad():\n",
        "    # First linear layer\n",
        "    hidden = ffn.net[0](x)\n",
        "    print(f\"After first linear: {hidden.shape}\")\n",
        "    print(f\"  Mean: {hidden.mean().item():.4f}, Std: {hidden.std().item():.4f}\")\n",
        "    \n",
        "    # After GELU\n",
        "    activated = ffn.net[1](hidden)\n",
        "    print(f\"After GELU: {activated.shape}\")\n",
        "    print(f\"  Mean: {activated.mean().item():.4f}, Std: {activated.std().item():.4f}\")\n",
        "    \n",
        "    # Final output\n",
        "    final = ffn.net[2](activated)\n",
        "    print(f\"Final output: {final.shape}\")\n",
        "    print(f\"  Mean: {final.mean().item():.4f}, Std: {final.std().item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Multi-Head Attention\n",
        "\n",
        "We've seen attention before, but let's examine it in the context of transformer blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create attention layer\n",
        "context_length = 32\n",
        "num_heads = 4\n",
        "\n",
        "attention = MultiHeadAttention(\n",
        "    input_dimension=embedding_dim,\n",
        "    output_dimension=embedding_dim,\n",
        "    context_length=context_length,\n",
        "    dropout=0.0,\n",
        "    number_of_heads=num_heads\n",
        ")\n",
        "\n",
        "print(f\"Multi-Head Attention:\")\n",
        "print(f\"  Input/Output dimension: {embedding_dim}\")\n",
        "print(f\"  Number of heads: {num_heads}\")\n",
        "print(f\"  Head dimension: {embedding_dim // num_heads}\")\n",
        "print(f\"  Context length: {context_length}\")\n",
        "\n",
        "# Test forward pass\n",
        "x = torch.randn(batch_size, seq_len, embedding_dim)\n",
        "attn_output = attention(x)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Output shape: {attn_output.shape}\")\n",
        "print(f\"Shape preserved: {x.shape == attn_output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Complete Transformer Block\n",
        "\n",
        "A transformer block combines attention and feed-forward with residual connections and layer normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a transformer block\n",
        "config = GPTConfig(\n",
        "    vocab_size=50257,\n",
        "    context_length=context_length,\n",
        "    embedding_dimension=embedding_dim,\n",
        "    number_of_heads=num_heads,\n",
        "    number_of_layers=1,  # Just for config\n",
        "    dropout_rate=0.0\n",
        ")\n",
        "\n",
        "transformer_block = TransformerBlock(config)\n",
        "\n",
        "print(\"Transformer Block components:\")\n",
        "print(f\"  - Multi-Head Attention\")\n",
        "print(f\"  - LayerNorm (x2)\")\n",
        "print(f\"  - Feed-Forward Network\")\n",
        "print(f\"  - Residual connections (x2)\")\n",
        "print(f\"  - Dropout (disabled for this example)\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in transformer_block.parameters())\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test transformer block forward pass\n",
        "x = torch.randn(batch_size, seq_len, embedding_dim)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Input mean: {x.mean().item():.4f}, std: {x.std().item():.4f}\")\n",
        "\n",
        "output = transformer_block(x)\n",
        "\n",
        "print(f\"\\nOutput shape: {output.shape}\")\n",
        "print(f\"Output mean: {output.mean().item():.4f}, std: {output.std().item():.4f}\")\n",
        "print(f\"Shape preserved: {x.shape == output.shape}\")\n",
        "\n",
        "# Verify residual connections work\n",
        "print(f\"\\nOutput is different from input (residuals added): {not torch.allclose(x, output)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Residual Connections\n",
        "\n",
        "Residual connections help with gradient flow and allow the model to learn identity mappings when needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate residual connection behavior\n",
        "x = torch.randn(1, 5, embedding_dim)\n",
        "\n",
        "# Manually trace through the transformer block to see residuals\n",
        "# This is a simplified view - actual implementation is in the block\n",
        "\n",
        "# First sub-layer: Attention with residual\n",
        "residual1 = x.clone()\n",
        "# In actual block: x = norm1(x), then attention, then x = x + residual1\n",
        "# For visualization, we'll show the concept\n",
        "\n",
        "print(\"Transformer Block Flow:\")\n",
        "print(f\"1. Input: {x.shape}\")\n",
        "print(f\"2. LayerNorm → Attention → Add residual\")\n",
        "print(f\"3. LayerNorm → FeedForward → Add residual\")\n",
        "print(f\"4. Output: {output.shape}\")\n",
        "\n",
        "# Show that output = input + transformation\n",
        "transformation = output - x\n",
        "print(f\"\\nTransformation magnitude: {transformation.norm().item():.4f}\")\n",
        "print(f\"Input magnitude: {x.norm().item():.4f}\")\n",
        "print(f\"Output magnitude: {output.norm().item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stacking Multiple Blocks\n",
        "\n",
        "Let's see how multiple transformer blocks process information sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create multiple transformer blocks\n",
        "num_layers = 3\n",
        "config = GPTConfig(\n",
        "    vocab_size=50257,\n",
        "    context_length=context_length,\n",
        "    embedding_dimension=embedding_dim,\n",
        "    number_of_heads=num_heads,\n",
        "    number_of_layers=num_layers,\n",
        "    dropout_rate=0.0\n",
        ")\n",
        "\n",
        "# Create blocks manually to track intermediate outputs\n",
        "blocks = [TransformerBlock(config) for _ in range(num_layers)]\n",
        "\n",
        "x = torch.randn(1, seq_len, embedding_dim)\n",
        "print(f\"Initial input shape: {x.shape}\")\n",
        "print(f\"Initial input norm: {x.norm().item():.4f}\\n\")\n",
        "\n",
        "# Process through each block\n",
        "intermediate_outputs = [x]\n",
        "for i, block in enumerate(blocks):\n",
        "    x = block(x)\n",
        "    intermediate_outputs.append(x)\n",
        "    print(f\"After block {i+1}: norm = {x.norm().item():.4f}, mean = {x.mean().item():.4f}\")\n",
        "\n",
        "print(f\"\\nFinal output shape: {x.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize how representations change through layers\n",
        "if len(intermediate_outputs) > 1:\n",
        "    # Compute statistics for each layer\n",
        "    norms = [out.norm().item() for out in intermediate_outputs]\n",
        "    means = [out.mean().item() for out in intermediate_outputs]\n",
        "    stds = [out.std().item() for out in intermediate_outputs]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    \n",
        "    layers = list(range(len(intermediate_outputs)))\n",
        "    \n",
        "    axes[0].plot(layers, norms, marker='o', linewidth=2, markersize=8)\n",
        "    axes[0].set_xlabel('Layer', fontsize=12)\n",
        "    axes[0].set_ylabel('Norm', fontsize=12)\n",
        "    axes[0].set_title('Output Norm Through Layers', fontsize=12, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    axes[1].plot(layers, means, marker='s', linewidth=2, markersize=8, color='orange')\n",
        "    axes[1].set_xlabel('Layer', fontsize=12)\n",
        "    axes[1].set_ylabel('Mean', fontsize=12)\n",
        "    axes[1].set_title('Output Mean Through Layers', fontsize=12, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
        "    \n",
        "    axes[2].plot(layers, stds, marker='^', linewidth=2, markersize=8, color='green')\n",
        "    axes[2].set_xlabel('Layer', fontsize=12)\n",
        "    axes[2].set_ylabel('Std', fontsize=12)\n",
        "    axes[2].set_title('Output Std Through Layers', fontsize=12, fontweight='bold')\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
