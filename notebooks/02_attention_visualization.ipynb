{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attention Mechanism Visualization\n",
        "\n",
        "This notebook provides an interactive guide to understanding this component of GPT.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add project root to path\n",
        "project_root = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "# Import our model classes\n",
        "from src.model.attention import MultiHeadAttention\n",
        "from src.model.gpt import GPTModel\n",
        "from src.config import GPTConfig\n",
        "from src.data.tokenizer import get_tokenizer\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Multi-Head Attention\n",
        "\n",
        "Attention is the core mechanism that allows transformers to understand relationships between tokens. This notebook visualizes how attention works in our GPT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple attention layer to explore\n",
        "embedding_dim = 64\n",
        "context_length = 32\n",
        "num_heads = 4\n",
        "\n",
        "attention = MultiHeadAttention(\n",
        "    input_dimension=embedding_dim,\n",
        "    output_dimension=embedding_dim,\n",
        "    context_length=context_length,\n",
        "    dropout=0.0,  # Disable dropout for visualization\n",
        "    number_of_heads=num_heads\n",
        ")\n",
        "\n",
        "print(f\"Attention layer created:\")\n",
        "print(f\"  Embedding dimension: {embedding_dim}\")\n",
        "print(f\"  Number of heads: {num_heads}\")\n",
        "print(f\"  Head dimension: {embedding_dim // num_heads}\")\n",
        "print(f\"  Context length: {context_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the causal mask\n",
        "print(\"Causal mask shape:\", attention.mask.shape)\n",
        "print(\"\\nCausal mask (first 10x10):\")\n",
        "print(attention.mask[:10, :10].int())\n",
        "print(\"\\nThe mask prevents tokens from attending to future tokens (upper triangle is True/1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple input and run attention\n",
        "batch_size = 1\n",
        "seq_len = 8\n",
        "x = torch.randn(batch_size, seq_len, embedding_dim)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "output = attention(x)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Input and output shapes match: {x.shape == output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting Attention Weights from a Model\n",
        "\n",
        "To visualize attention, we need to extract the attention weights during the forward pass. We'll use PyTorch hooks to capture these weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hook class to capture attention weights\n",
        "class AttentionHook:\n",
        "    \"\"\"Hook to capture attention weights from MultiHeadAttention module.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.attention_weights = []\n",
        "        self.layer_idx = 0\n",
        "    \n",
        "    def __call__(self, module, input, output):\n",
        "        \"\"\"Capture attention weights during forward pass.\"\"\"\n",
        "        x = input[0]  # Input tensor\n",
        "        \n",
        "        # Get Q, K, V\n",
        "        queries = module.W_query(x)\n",
        "        keys = module.W_key(x)\n",
        "        values = module.W_value(x)\n",
        "        \n",
        "        batch_size, num_tokens, _ = x.shape\n",
        "        \n",
        "        # Split into heads\n",
        "        queries = queries.view(batch_size, num_tokens, module.number_of_heads, module.head_dimension)\n",
        "        queries = queries.transpose(1, 2)  # [batch, heads, tokens, head_dimension]\n",
        "        \n",
        "        keys = keys.view(batch_size, num_tokens, module.number_of_heads, module.head_dimension)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        attention_scores = queries @ keys.transpose(-2, -1)  # [batch, heads, tokens, tokens]\n",
        "        \n",
        "        # Apply mask\n",
        "        mask = module.mask[:num_tokens, :num_tokens]\n",
        "        attention_scores = attention_scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "        \n",
        "        # Apply scaling and softmax\n",
        "        scaling_factor = module.head_dimension ** 0.5\n",
        "        attention_weights = torch.softmax(attention_scores / scaling_factor, dim=-1)\n",
        "        \n",
        "        # Store weights (remove batch dimension, take first sample)\n",
        "        self.attention_weights.append(attention_weights[0].detach().cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a small GPT model\n",
        "config = GPTConfig(\n",
        "    vocab_size=50257,\n",
        "    context_length=128,\n",
        "    embedding_dimension=128,\n",
        "    number_of_heads=4,\n",
        "    number_of_layers=2,\n",
        "    dropout_rate=0.0  # Disable dropout for visualization\n",
        ")\n",
        "\n",
        "model = GPTModel(config)\n",
        "model.eval()\n",
        "print(f\"Model created with {config.number_of_layers} layers and {config.number_of_heads} heads per layer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare input text\n",
        "tokenizer = get_tokenizer(\"gpt2\")\n",
        "text = \"The cat sat on the mat. It was fluffy.\"\n",
        "token_ids = tokenizer.encode(text)\n",
        "tokens = [tokenizer.decode([tid]) for tid in token_ids]\n",
        "\n",
        "print(f\"Text: '{text}'\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Number of tokens: {len(tokens)}\")\n",
        "\n",
        "# Convert to tensor\n",
        "input_ids = torch.tensor([token_ids], dtype=torch.long)\n",
        "print(f\"Input shape: {input_ids.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register hooks to capture attention weights\n",
        "hooks = []\n",
        "attention_hooks = []\n",
        "\n",
        "for i, block in enumerate(model.transformer_blocks):\n",
        "    hook = AttentionHook()\n",
        "    hook.layer_idx = i\n",
        "    attention_hooks.append(hook)\n",
        "    \n",
        "    # Register forward hook\n",
        "    handle = block.attention.register_forward_hook(hook)\n",
        "    hooks.append(handle)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    _ = model(input_ids)\n",
        "\n",
        "# Extract attention weights\n",
        "all_weights = []\n",
        "for hook in attention_hooks:\n",
        "    if len(hook.attention_weights) > 0:\n",
        "        all_weights.append(hook.attention_weights[0])\n",
        "    else:\n",
        "        all_weights.append(None)\n",
        "\n",
        "# Remove hooks\n",
        "for handle in hooks:\n",
        "    handle.remove()\n",
        "\n",
        "print(f\"Extracted attention from {len(all_weights)} layers\")\n",
        "if all_weights[0] is not None:\n",
        "    print(f\"Shape per layer: {all_weights[0].shape}  # [num_heads, seq_len, seq_len]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Attention Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_attention_head(attention_weights, tokens, layer_idx, head_idx, ax=None):\n",
        "    \"\"\"Visualize attention weights for a single head.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    \n",
        "    # Create heatmap\n",
        "    im = ax.imshow(attention_weights, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
        "    \n",
        "    # Set ticks and labels\n",
        "    ax.set_xticks(range(len(tokens)))\n",
        "    ax.set_yticks(range(len(tokens)))\n",
        "    ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)\n",
        "    ax.set_yticklabels(tokens, fontsize=8)\n",
        "    \n",
        "    # Add colorbar\n",
        "    plt.colorbar(im, ax=ax, label='Attention Weight')\n",
        "    \n",
        "    # Title\n",
        "    ax.set_title(f'Layer {layer_idx}, Head {head_idx}', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Key (Attended To)', fontsize=10)\n",
        "    ax.set_ylabel('Query (Attending From)', fontsize=10)\n",
        "    \n",
        "    return ax\n",
        "\n",
        "# Visualize first head of first layer\n",
        "if all_weights[0] is not None:\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    visualize_attention_head(all_weights[0][0], tokens, 0, 0, ax=ax)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize all heads for a layer\n",
        "def visualize_all_heads(attention_weights, tokens, layer_idx):\n",
        "    \"\"\"Visualize all heads for a layer in a grid.\"\"\"\n",
        "    num_heads = attention_weights.shape[0]\n",
        "    cols = 2\n",
        "    rows = (num_heads + cols - 1) // cols\n",
        "    \n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(12, 5 * rows))\n",
        "    if rows == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for head_idx in range(num_heads):\n",
        "        row = head_idx // cols\n",
        "        col = head_idx % cols\n",
        "        ax = axes[row, col]\n",
        "        \n",
        "        visualize_attention_head(\n",
        "            attention_weights[head_idx],\n",
        "            tokens,\n",
        "            layer_idx,\n",
        "            head_idx,\n",
        "            ax=ax\n",
        "        )\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for head_idx in range(num_heads, rows * cols):\n",
        "        row = head_idx // cols\n",
        "        col = head_idx % cols\n",
        "        axes[row, col].axis('off')\n",
        "    \n",
        "    plt.suptitle(f'All Attention Heads - Layer {layer_idx}', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Visualize all heads of first layer\n",
        "if all_weights[0] is not None:\n",
        "    fig = visualize_all_heads(all_weights[0], tokens, 0)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare attention patterns across layers\n",
        "if len(all_weights) > 1 and all_weights[0] is not None:\n",
        "    # Average across all heads for each layer\n",
        "    average_weights = [w.mean(axis=0) for w in all_weights if w is not None]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, len(average_weights), figsize=(6 * len(average_weights), 6))\n",
        "    if len(average_weights) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for layer_idx, avg_weights in enumerate(average_weights):\n",
        "        im = axes[layer_idx].imshow(avg_weights, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
        "        axes[layer_idx].set_xticks(range(len(tokens)))\n",
        "        axes[layer_idx].set_yticks(range(len(tokens)))\n",
        "        axes[layer_idx].set_xticklabels(tokens, rotation=45, ha='right', fontsize=8)\n",
        "        axes[layer_idx].set_yticklabels(tokens, fontsize=8)\n",
        "        axes[layer_idx].set_title(f'Layer {layer_idx} (Averaged)', fontsize=12, fontweight='bold')\n",
        "        plt.colorbar(im, ax=axes[layer_idx], label='Attention Weight')\n",
        "    \n",
        "    plt.suptitle('Attention Patterns Across Layers', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing Attention Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze which tokens attend to which other tokens\n",
        "if all_weights[0] is not None:\n",
        "    # Average across all heads\n",
        "    avg_weights = all_weights[0].mean(axis=0)  # [seq_len, seq_len]\n",
        "    \n",
        "    print(\"Top attention targets for each token (Layer 0, averaged across heads):\\n\")\n",
        "    for token_idx, token in enumerate(tokens):\n",
        "        # Get attention from this token to all others (only previous tokens due to causal mask)\n",
        "        attention_from_token = avg_weights[token_idx, :token_idx+1]\n",
        "        \n",
        "        if len(attention_from_token) > 1:  # Not just self-attention\n",
        "            # Get top-3 attended tokens\n",
        "            top_indices = np.argsort(attention_from_token)[-3:][::-1]\n",
        "            top_weights = attention_from_token[top_indices]\n",
        "            \n",
        "            print(f\"'{token}' attends most to:\")\n",
        "            for i, (idx, weight) in enumerate(zip(top_indices, top_weights)):\n",
        "                if idx < len(tokens):\n",
        "                    print(f\"  {i+1}. '{tokens[idx]}' (weight: {weight:.3f})\")\n",
        "            print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot attention flow across layers\n",
        "if len(all_weights) > 1:\n",
        "    average_weights = [w.mean(axis=0) for w in all_weights if w is not None]\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    layers = list(range(len(average_weights)))\n",
        "    # Average self-attention (diagonal) across all tokens for each layer\n",
        "    self_attention = [np.mean(np.diag(avg_weights)) for avg_weights in average_weights]\n",
        "    \n",
        "    ax.plot(layers, self_attention, marker='o', linewidth=2, markersize=8, label='Average Self-Attention')\n",
        "    \n",
        "    # Also plot average attention to previous tokens\n",
        "    previous_attention = []\n",
        "    for avg_weights in average_weights:\n",
        "        # Get attention to previous tokens (lower triangle, excluding diagonal)\n",
        "        mask = np.tril(np.ones_like(avg_weights), k=-1).astype(bool)\n",
        "        previous_attention.append(np.mean(avg_weights[mask]) if np.any(mask) else 0.0)\n",
        "    \n",
        "    ax.plot(layers, previous_attention, marker='s', linewidth=2, markersize=8, \n",
        "            label='Average Attention to Previous Tokens')\n",
        "    \n",
        "    ax.set_xlabel('Layer', fontsize=12)\n",
        "    ax.set_ylabel('Attention Weight', fontsize=12)\n",
        "    ax.set_title('Attention Patterns Across Layers', fontsize=14, fontweight='bold')\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_xticks(layers)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
