{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training From Scratch\n",
        "\n",
        "This notebook provides an interactive guide to understanding this component of GPT.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Add project root to path\n",
        "project_root = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "# Import our training components\n",
        "from src.model.gpt import GPTModel\n",
        "from src.config import GPTConfig\n",
        "from src.data.dataset import GPTDataset\n",
        "from src.data.tokenizer import get_tokenizer\n",
        "from src.training.trainer import GPTTrainer\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training GPT From Scratch\n",
        "\n",
        "This notebook demonstrates how to train a GPT model from scratch using the training utilities provided in this codebase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load or create sample text\n",
        "sample_text_path = os.path.join(project_root, \"data\", \"sample_text.txt\")\n",
        "if os.path.exists(sample_text_path):\n",
        "    with open(sample_text_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    print(f\"Loaded text: {len(text):,} characters\")\n",
        "else:\n",
        "    # Use a simple example\n",
        "    text = \"\"\"Once upon a time, there was a little girl named Emma. She loved to play in the garden. \n",
        "Every morning, Emma would wake up early and run outside. She would pick flowers and watch the butterflies. \n",
        "The garden was her favorite place in the whole world. She spent hours there every day, playing and exploring.\n",
        "The cat sat on the mat. It was a sunny day. The cat was very happy. It purred softly and stretched its paws.\"\"\"\n",
        "    print(\"Using fallback text\")\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = get_tokenizer(\"gpt2\")\n",
        "vocab_size = tokenizer.n_vocab\n",
        "print(f\"Vocabulary size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "context_length = 32  # Small for demonstration\n",
        "stride = context_length // 2  # 50% overlap\n",
        "\n",
        "dataset = GPTDataset(\n",
        "    text=text,\n",
        "    tokenizer=tokenizer,\n",
        "    maximum_length=context_length,\n",
        "    stride=stride\n",
        ")\n",
        "\n",
        "print(f\"Dataset created:\")\n",
        "print(f\"  Total sequences: {len(dataset):,}\")\n",
        "print(f\"  Context length: {context_length}\")\n",
        "print(f\"  Stride: {stride}\")\n",
        "\n",
        "# Split into train/validation\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "if train_size == 0:\n",
        "    train_size = 1\n",
        "    val_size = len(dataset) - 1\n",
        "if val_size == 0:\n",
        "    val_size = 1\n",
        "    train_size = len(dataset) - 1\n",
        "\n",
        "train_dataset, val_dataset = random_split(\n",
        "    dataset, [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain sequences: {len(train_dataset):,}\")\n",
        "print(f\"Validation sequences: {len(val_dataset):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data loaders\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"Data loaders created:\")\n",
        "print(f\"  Batch size: {batch_size}\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Validation batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a small model for training\n",
        "config = GPTConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    context_length=context_length,\n",
        "    embedding_dimension=128,\n",
        "    number_of_heads=4,\n",
        "    number_of_layers=2,\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "model = GPTModel(config)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model created:\")\n",
        "print(f\"  Parameters: {total_params:,}\")\n",
        "print(f\"  Model size: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"  Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Setup Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create optimizer\n",
        "learning_rate = 3e-4\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    weight_decay=0.1,\n",
        "    betas=(0.9, 0.95)\n",
        ")\n",
        "\n",
        "print(f\"Optimizer: AdamW\")\n",
        "print(f\"  Learning rate: {learning_rate}\")\n",
        "print(f\"  Weight decay: 0.1\")\n",
        "\n",
        "# Create trainer\n",
        "trainer = GPTTrainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"\\nTrainer created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train for a few epochs\n",
        "num_epochs = 3\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(f\"Training for {num_epochs} epochs...\\n\")\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Train\n",
        "    train_loss = trainer.train_epoch()\n",
        "    train_perplexity = torch.exp(torch.tensor(train_loss)).item()\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    print(f\"Train Loss: {train_loss:.4f} | Perplexity: {train_perplexity:.2f}\")\n",
        "    \n",
        "    # Validate\n",
        "    val_loss = trainer.validate()\n",
        "    if val_loss is not None:\n",
        "        val_perplexity = torch.exp(torch.tensor(val_loss)).item()\n",
        "        val_losses.append(val_loss)\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Perplexity: {val_perplexity:.2f}\")\n",
        "    \n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "if len(train_losses) > 0:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    epochs = list(range(1, len(train_losses) + 1))\n",
        "    \n",
        "    # Loss plot\n",
        "    axes[0].plot(epochs, train_losses, marker='o', label='Train Loss', linewidth=2)\n",
        "    if len(val_losses) > 0:\n",
        "        axes[0].plot(epochs, val_losses, marker='s', label='Val Loss', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Perplexity plot\n",
        "    train_perplexities = [torch.exp(torch.tensor(l)).item() for l in train_losses]\n",
        "    axes[1].plot(epochs, train_perplexities, marker='o', label='Train Perplexity', linewidth=2)\n",
        "    if len(val_losses) > 0:\n",
        "        val_perplexities = [torch.exp(torch.tensor(l)).item() for l in val_losses]\n",
        "        axes[1].plot(epochs, val_perplexities, marker='s', label='Val Perplexity', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Perplexity', fontsize=12)\n",
        "    axes[1].set_title('Training and Validation Perplexity', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Understanding the Loss\n",
        "\n",
        "The model is trained to predict the next token at each position. The loss is computed using CrossEntropyLoss between predictions and targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine a training batch to understand the loss computation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Get a batch\n",
        "    input_ids, target_ids = next(iter(train_loader))\n",
        "    input_ids = input_ids.to(device)\n",
        "    target_ids = target_ids.to(device)\n",
        "    \n",
        "    # Forward pass\n",
        "    logits = model(input_ids)  # [batch_size, seq_len, vocab_size]\n",
        "    \n",
        "    # Compute loss manually\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    loss = criterion(\n",
        "        logits.view(-1, logits.size(-1)),  # [batch*seq_len, vocab_size]\n",
        "        target_ids.view(-1)  # [batch*seq_len]\n",
        "    )\n",
        "    \n",
        "    print(f\"Batch example:\")\n",
        "    print(f\"  Input shape: {input_ids.shape}\")\n",
        "    print(f\"  Target shape: {target_ids.shape}\")\n",
        "    print(f\"  Logits shape: {logits.shape}\")\n",
        "    print(f\"  Loss: {loss.item():.4f}\")\n",
        "    print(f\"  Perplexity: {torch.exp(loss).item():.2f}\")\n",
        "    \n",
        "    # Show predictions for first sequence\n",
        "    first_seq_logits = logits[0]  # [seq_len, vocab_size]\n",
        "    first_seq_preds = torch.argmax(first_seq_logits, dim=-1)\n",
        "    first_seq_targets = target_ids[0]\n",
        "    \n",
        "    print(f\"\\nFirst sequence:\")\n",
        "    print(f\"  Input tokens: {input_ids[0].tolist()[:10]}...\")\n",
        "    print(f\"  Target tokens: {first_seq_targets.tolist()[:10]}...\")\n",
        "    print(f\"  Predicted tokens: {first_seq_preds.tolist()[:10]}...\")\n",
        "    print(f\"  Accuracy: {(first_seq_preds == first_seq_targets).float().mean().item():.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Saving and Loading Checkpoints\n",
        "\n",
        "It's important to save model checkpoints during training for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save checkpoint\n",
        "checkpoint_dir = os.path.join(project_root, \"checkpoints\")\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"notebook_checkpoint.pt\")\n",
        "checkpoint = {\n",
        "    'epoch': num_epochs,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'config': config.to_dict(),\n",
        "    'train_loss': train_losses[-1] if train_losses else None,\n",
        "    'val_loss': val_losses[-1] if val_losses else None,\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, checkpoint_path)\n",
        "print(f\"Checkpoint saved to: {checkpoint_path}\")\n",
        "\n",
        "# Load checkpoint to verify\n",
        "loaded_checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
        "print(f\"\\nCheckpoint loaded:\")\n",
        "print(f\"  Epoch: {loaded_checkpoint['epoch']}\")\n",
        "print(f\"  Train loss: {loaded_checkpoint.get('train_loss', 'N/A')}\")\n",
        "print(f\"  Val loss: {loaded_checkpoint.get('val_loss', 'N/A')}\")\n",
        "\n",
        "# Create a new model and load weights\n",
        "new_model = GPTModel(config)\n",
        "new_model.load_state_dict(loaded_checkpoint['model_state_dict'])\n",
        "print(f\"\\nModel weights loaded successfully!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
