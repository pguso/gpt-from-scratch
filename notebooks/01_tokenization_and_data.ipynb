{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenization and Data Preparation\n",
        "\n",
        "This notebook provides an interactive guide to understanding this component of GPT.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add project root to path\n",
        "project_root = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "# Import our tokenizer and dataset classes\n",
        "from src.data.tokenizer import get_tokenizer\n",
        "from src.data.dataset import GPTDataset\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Tokenization\n",
        "\n",
        "Tokenization is the process of converting raw text into numerical tokens that the model can process. We use the GPT-2 tokenizer (via tiktoken) which uses Byte Pair Encoding (BPE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the tokenizer\n",
        "tokenizer = get_tokenizer(\"gpt2\")\n",
        "print(f\"Vocabulary size: {tokenizer.n_vocab}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Tokenize a simple sentence\n",
        "text = \"Hello, world! This is GPT from scratch.\"\n",
        "tokens = tokenizer.encode(text)\n",
        "print(f\"Text: {text}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Number of tokens: {len(tokens)}\")\n",
        "\n",
        "# Decode back to text\n",
        "decoded = tokenizer.decode(tokens)\n",
        "print(f\"Decoded: {decoded}\")\n",
        "print(f\"Round-trip successful: {decoded == text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize tokenization: see how words are split\n",
        "example_texts = [\n",
        "    \"Hello\",\n",
        "    \"Hello world\",\n",
        "    \"The quick brown fox\",\n",
        "    \"GPT-2 uses Byte Pair Encoding\",\n",
        "    \"tokenization\"\n",
        "]\n",
        "\n",
        "for text in example_texts:\n",
        "    tokens = tokenizer.encode(text)\n",
        "    token_strings = [tokenizer.decode([t]) for t in tokens]\n",
        "    print(f\"'{text}' â†’ {tokens}\")\n",
        "    print(f\"  Tokens: {token_strings}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Dataset\n",
        "\n",
        "The `GPTDataset` class creates training sequences using a sliding window approach. For each position, we create input-target pairs where the target is shifted by one position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sample text\n",
        "sample_text_path = os.path.join(project_root, \"data\", \"sample_text.txt\")\n",
        "if os.path.exists(sample_text_path):\n",
        "    with open(sample_text_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    print(f\"Loaded text: {len(text)} characters\")\n",
        "    print(f\"First 200 characters: {text[:200]}...\")\n",
        "else:\n",
        "    # Fallback to a simple example\n",
        "    text = \"Once upon a time, there was a little girl named Emma. She loved to play in the garden.\"\n",
        "    print(\"Using fallback text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset with sliding window\n",
        "context_length = 16  # Maximum sequence length\n",
        "stride = 8  # Step size for sliding window (50% overlap)\n",
        "\n",
        "dataset = GPTDataset(\n",
        "    text=text,\n",
        "    tokenizer=tokenizer,\n",
        "    maximum_length=context_length,\n",
        "    stride=stride\n",
        ")\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)} sequences\")\n",
        "print(f\"Context length: {context_length}\")\n",
        "print(f\"Stride: {stride}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine a few examples\n",
        "print(\"Example sequences from the dataset:\\n\")\n",
        "for i in range(min(3, len(dataset))):\n",
        "    input_ids, target_ids = dataset[i]\n",
        "    \n",
        "    # Decode to see the text\n",
        "    input_text = tokenizer.decode(input_ids.tolist())\n",
        "    target_text = tokenizer.decode(target_ids.tolist())\n",
        "    \n",
        "    print(f\"Sequence {i}:\")\n",
        "    print(f\"  Input:  {input_text}\")\n",
        "    print(f\"  Target: {target_text}\")\n",
        "    print(f\"  Input tokens:  {input_ids.tolist()}\")\n",
        "    print(f\"  Target tokens: {target_ids.tolist()}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify that targets are shifted by one position\n",
        "input_ids, target_ids = dataset[0]\n",
        "print(\"Verifying target shift:\")\n",
        "print(f\"Input:  {input_ids.tolist()}\")\n",
        "print(f\"Target: {target_ids.tolist()}\")\n",
        "print(f\"\\nTarget should be input shifted by 1:\")\n",
        "print(f\"Input[1:] == Target[:-1]: {(input_ids[1:] == target_ids[:-1]).all().item()}\")\n",
        "print(f\"Target[-1] is the next token after input[-1]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Dataset Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze token distribution\n",
        "all_tokens = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "unique_tokens = len(set(all_tokens))\n",
        "vocab_usage = unique_tokens / tokenizer.n_vocab * 100\n",
        "\n",
        "print(f\"Text statistics:\")\n",
        "print(f\"  Total tokens: {len(all_tokens):,}\")\n",
        "print(f\"  Unique tokens: {unique_tokens:,}\")\n",
        "print(f\"  Vocabulary usage: {vocab_usage:.2f}%\")\n",
        "print(f\"  Average tokens per character: {len(all_tokens) / len(text):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot token frequency distribution\n",
        "token_counts = {}\n",
        "for token in all_tokens:\n",
        "    token_counts[token] = token_counts.get(token, 0) + 1\n",
        "\n",
        "# Get top 20 most frequent tokens\n",
        "top_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "token_ids, counts = zip(*top_tokens)\n",
        "token_strings = [tokenizer.decode([tid]) for tid in token_ids]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(len(token_strings)), counts)\n",
        "plt.xticks(range(len(token_strings)), token_strings, rotation=45, ha='right')\n",
        "plt.xlabel('Token')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Most Frequent Tokens')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing with PyTorch DataLoader\n",
        "\n",
        "Let's see how the dataset works with PyTorch's DataLoader for batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 4\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Get a batch\n",
        "for batch_idx, (input_batch, target_batch) in enumerate(dataloader):\n",
        "    print(f\"Batch {batch_idx}:\")\n",
        "    print(f\"  Input shape: {input_batch.shape}\")  # [batch_size, sequence_length]\n",
        "    print(f\"  Target shape: {target_batch.shape}\")  # [batch_size, sequence_length]\n",
        "    print(f\"  Data types: input={input_batch.dtype}, target={target_batch.dtype}\")\n",
        "    \n",
        "    # Show first sequence in batch\n",
        "    first_input = input_batch[0].tolist()\n",
        "    first_target = target_batch[0].tolist()\n",
        "    print(f\"  First sequence input: {tokenizer.decode(first_input)}\")\n",
        "    print(f\"  First sequence target: {tokenizer.decode(first_target)}\")\n",
        "    print()\n",
        "    \n",
        "    if batch_idx >= 1:  # Just show first 2 batches\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
