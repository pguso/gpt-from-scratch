{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete GPT Model\n",
        "\n",
        "This notebook provides an interactive guide to understanding this component of GPT.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add project root to path\n",
        "project_root = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "# Import our GPT model and components\n",
        "from src.model.gpt import GPTModel, TransformerBlock\n",
        "from src.model.embeddings import TokenEmbedding, PositionalEmbedding\n",
        "from src.config import GPTConfig\n",
        "from src.data.tokenizer import get_tokenizer\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete GPT Model\n",
        "\n",
        "This notebook demonstrates how all components come together to form a complete GPT model: embeddings, transformer blocks, and output layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Token and Positional Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create embeddings\n",
        "vocab_size = 50257\n",
        "embedding_dim = 128\n",
        "context_length = 128\n",
        "\n",
        "token_embedding = TokenEmbedding(vocab_size, embedding_dim)\n",
        "position_embedding = PositionalEmbedding(context_length, embedding_dim)\n",
        "\n",
        "print(f\"Token Embedding:\")\n",
        "print(f\"  Vocabulary size: {vocab_size}\")\n",
        "print(f\"  Embedding dimension: {embedding_dim}\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in token_embedding.parameters()):,}\")\n",
        "\n",
        "print(f\"\\nPositional Embedding:\")\n",
        "print(f\"  Context length: {context_length}\")\n",
        "print(f\"  Embedding dimension: {embedding_dim}\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in position_embedding.parameters()):,}\")\n",
        "\n",
        "# Test embeddings\n",
        "token_ids = torch.tensor([[1, 2, 3, 4, 5]])\n",
        "positions = torch.arange(5)\n",
        "\n",
        "token_embeds = token_embedding(token_ids)\n",
        "pos_embeds = position_embedding(positions)\n",
        "\n",
        "print(f\"\\nToken embeddings shape: {token_embeds.shape}\")\n",
        "print(f\"Position embeddings shape: {pos_embeds.shape}\")\n",
        "\n",
        "# Combined embeddings\n",
        "combined = token_embeds + pos_embeds.unsqueeze(0)\n",
        "print(f\"Combined embeddings shape: {combined.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Creating a GPT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a small GPT model configuration\n",
        "config = GPTConfig(\n",
        "    vocab_size=50257,\n",
        "    context_length=128,\n",
        "    embedding_dimension=256,\n",
        "    number_of_heads=4,\n",
        "    number_of_layers=4,\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "print(\"GPT Model Configuration:\")\n",
        "print(f\"  Vocabulary size: {config.vocab_size}\")\n",
        "print(f\"  Context length: {config.context_length}\")\n",
        "print(f\"  Embedding dimension: {config.embedding_dimension}\")\n",
        "print(f\"  Number of heads: {config.number_of_heads}\")\n",
        "print(f\"  Number of layers: {config.number_of_layers}\")\n",
        "print(f\"  Dropout rate: {config.dropout_rate}\")\n",
        "\n",
        "# Create the model\n",
        "model = GPTModel(config)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  Model size (FP32): {total_params * 4 / 1024 / 1024:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Break down parameter count by component\n",
        "print(\"Parameter breakdown:\")\n",
        "print(f\"  Token embedding: {sum(p.numel() for p in model.token_embedding.parameters()):,}\")\n",
        "print(f\"  Position embedding: {sum(p.numel() for p in model.position_embedding.parameters()):,}\")\n",
        "print(f\"  Transformer blocks: {sum(p.numel() for p in model.transformer_blocks.parameters()):,}\")\n",
        "print(f\"  Final layer norm: {sum(p.numel() for p in model.final_norm.parameters()):,}\")\n",
        "print(f\"  Language model head: {sum(p.numel() for p in model.lm_head.parameters()):,}\")\n",
        "\n",
        "# Verify\n",
        "total_breakdown = (\n",
        "    sum(p.numel() for p in model.token_embedding.parameters()) +\n",
        "    sum(p.numel() for p in model.position_embedding.parameters()) +\n",
        "    sum(p.numel() for p in model.transformer_blocks.parameters()) +\n",
        "    sum(p.numel() for p in model.final_norm.parameters()) +\n",
        "    sum(p.numel() for p in model.lm_head.parameters())\n",
        ")\n",
        "print(f\"\\nTotal from breakdown: {total_breakdown:,}\")\n",
        "print(f\"Matches total: {total_breakdown == total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare input\n",
        "tokenizer = get_tokenizer(\"gpt2\")\n",
        "text = \"The quick brown fox\"\n",
        "token_ids = tokenizer.encode(text)\n",
        "\n",
        "print(f\"Text: '{text}'\")\n",
        "print(f\"Token IDs: {token_ids}\")\n",
        "print(f\"Number of tokens: {len(token_ids)}\")\n",
        "\n",
        "# Convert to tensor\n",
        "input_ids = torch.tensor([token_ids], dtype=torch.long)\n",
        "print(f\"Input shape: {input_ids.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(input_ids)\n",
        "\n",
        "print(f\"\\nOutput logits shape: {logits.shape}\")\n",
        "print(f\"  Batch size: {logits.shape[0]}\")\n",
        "print(f\"  Sequence length: {logits.shape[1]}\")\n",
        "print(f\"  Vocabulary size: {logits.shape[2]}\")\n",
        "\n",
        "# Get predictions\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "predicted_token_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "print(f\"\\nPredicted token IDs: {predicted_token_ids[0].tolist()}\")\n",
        "print(f\"Predicted tokens: {[tokenizer.decode([tid]) for tid in predicted_token_ids[0].tolist()]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Understanding the Output\n",
        "\n",
        "The model outputs logits (unnormalized probabilities) for each position in the sequence. These represent the model's prediction for the next token at each position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the logits for the last position (next token prediction)\n",
        "last_position_logits = logits[0, -1, :]  # [vocab_size]\n",
        "last_position_probs = torch.softmax(last_position_logits, dim=-1)\n",
        "\n",
        "# Get top-10 most likely next tokens\n",
        "top_k = 10\n",
        "top_probs, top_indices = torch.topk(last_position_probs, top_k)\n",
        "\n",
        "print(f\"Top {top_k} most likely next tokens:\")\n",
        "for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
        "    token = tokenizer.decode([idx.item()])\n",
        "    print(f\"  {i+1}. '{token}' (probability: {prob.item():.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the probability distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "sorted_probs, sorted_indices = torch.sort(last_position_probs, descending=True)\n",
        "top_20_probs = sorted_probs[:20]\n",
        "top_20_indices = sorted_indices[:20]\n",
        "top_20_tokens = [tokenizer.decode([idx.item()]) for idx in top_20_indices]\n",
        "\n",
        "plt.bar(range(len(top_20_probs)), top_20_probs.numpy())\n",
        "plt.xticks(range(len(top_20_tokens)), top_20_tokens, rotation=45, ha='right')\n",
        "plt.xlabel('Token', fontsize=12)\n",
        "plt.ylabel('Probability', fontsize=12)\n",
        "plt.title('Top 20 Next Token Predictions (Untrained Model)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Note: This is an untrained model, so predictions are essentially random.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Model Architecture Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print model architecture\n",
        "print(\"GPT Model Architecture:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n1. Input: Token IDs [batch_size, sequence_length]\")\n",
        "print(\"\\n2. Token Embedding:\")\n",
        "print(f\"   - Maps {config.vocab_size} tokens to {config.embedding_dimension}D vectors\")\n",
        "print(\"\\n3. Positional Embedding:\")\n",
        "print(f\"   - Adds position information (max {config.context_length} positions)\")\n",
        "print(\"\\n4. Combined Embeddings:\")\n",
        "print(\"   - token_embedding + positional_embedding\")\n",
        "print(f\"   - Shape: [batch_size, sequence_length, {config.embedding_dimension}]\")\n",
        "print(\"\\n5. Transformer Blocks (x{}):\".format(config.number_of_layers))\n",
        "print(\"   - Multi-Head Attention\")\n",
        "print(\"   - Feed-Forward Network\")\n",
        "print(\"   - Layer Normalization (x2)\")\n",
        "print(\"   - Residual Connections (x2)\")\n",
        "print(\"\\n6. Final Layer Normalization\")\n",
        "print(\"\\n7. Language Model Head:\")\n",
        "print(f\"   - Linear layer: {config.embedding_dimension} â†’ {config.vocab_size}\")\n",
        "print(\"\\n8. Output: Logits [batch_size, sequence_length, vocab_size]\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Testing with Different Input Sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with different sequence lengths\n",
        "test_texts = [\n",
        "    \"Hello\",\n",
        "    \"The quick brown fox jumps\",\n",
        "    \"Once upon a time, in a land far away, there lived\"\n",
        "]\n",
        "\n",
        "print(\"Testing with different input lengths:\\n\")\n",
        "for text in test_texts:\n",
        "    token_ids = tokenizer.encode(text)\n",
        "    input_ids = torch.tensor([token_ids], dtype=torch.long)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids)\n",
        "    \n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"  Input length: {len(token_ids)} tokens\")\n",
        "    print(f\"  Output shape: {logits.shape}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Model Components Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all model components\n",
        "print(\"Model Components:\")\n",
        "print(\"-\" * 60)\n",
        "for name, module in model.named_children():\n",
        "    num_params = sum(p.numel() for p in module.parameters())\n",
        "    print(f\"{name:20s} : {num_params:>12,} parameters\")\n",
        "    if hasattr(module, '__len__'):\n",
        "        print(f\"{'':20s}   ({len(module)} sub-modules)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Total: {total_params:>12,} parameters\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
