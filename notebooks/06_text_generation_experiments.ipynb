{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Generation Experiments\n",
        "\n",
        "This notebook provides an interactive guide to understanding this component of GPT.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add project root to path\n",
        "project_root = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "# Import generation utilities\n",
        "from src.model.gpt import GPTModel\n",
        "from src.config import GPTConfig\n",
        "from src.generation.generate import generate_text\n",
        "from src.data.tokenizer import get_tokenizer\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Generation Experiments\n",
        "\n",
        "This notebook explores different text generation strategies: greedy decoding, temperature sampling, and top-k sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Load or Create a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to load a trained model, or create a new one\n",
        "checkpoint_path = os.path.join(project_root, \"checkpoints\", \"best_model.pt\")\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Loading trained model from {checkpoint_path}...\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)\n",
        "    \n",
        "    if 'config' in checkpoint:\n",
        "        config = GPTConfig(**checkpoint['config'])\n",
        "    else:\n",
        "        config = GPTConfig(\n",
        "            vocab_size=50257,\n",
        "            context_length=128,\n",
        "            embedding_dimension=256,\n",
        "            number_of_heads=4,\n",
        "            number_of_layers=4\n",
        "        )\n",
        "    \n",
        "    model = GPTModel(config)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    print(\"Trained model loaded!\")\n",
        "else:\n",
        "    print(\"No trained model found. Creating a new untrained model...\")\n",
        "    config = GPTConfig(\n",
        "        vocab_size=50257,\n",
        "        context_length=128,\n",
        "        embedding_dimension=256,\n",
        "        number_of_heads=4,\n",
        "        number_of_layers=4\n",
        "    )\n",
        "    model = GPTModel(config)\n",
        "    model.eval()\n",
        "    print(\"Note: Untrained model will generate random text\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = get_tokenizer(\"gpt2\")\n",
        "print(f\"Tokenizer vocabulary size: {tokenizer.n_vocab}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Basic Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text with default settings\n",
        "prompt = \"Once upon a time\"\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\")\n",
        "print(f\"Input tokens: {input_ids}\")\n",
        "\n",
        "# Generate\n",
        "output_ids = generate_text(\n",
        "    model,\n",
        "    input_ids,\n",
        "    maximum_new_tokens=20,\n",
        "    temperature=1.0,\n",
        "    top_k_tokens=None\n",
        ")\n",
        "\n",
        "output_text = tokenizer.decode(output_ids)\n",
        "print(f\"\\nGenerated text: {output_text}\")\n",
        "print(f\"Generated tokens: {len(output_ids) - len(input_ids)} new tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Temperature Sampling\n",
        "\n",
        "Temperature controls the randomness of generation. Lower temperature = more deterministic, higher = more random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different temperature settings\n",
        "prompt = \"The cat sat\"\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "temperatures = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\\n\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for temp in temperatures:\n",
        "    output_ids = generate_text(\n",
        "        model,\n",
        "        input_ids,\n",
        "        maximum_new_tokens=15,\n",
        "        temperature=temp,\n",
        "        top_k_tokens=None\n",
        "    )\n",
        "    output_text = tokenizer.decode(output_ids)\n",
        "    print(f\"Temperature {temp:3.1f}: {output_text}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize how temperature affects the probability distribution\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Get logits for a single token\n",
        "    test_input = torch.tensor([[tokenizer.encode(\"The\")[0]]], device=device)\n",
        "    logits = model(test_input)[0, -1, :]  # [vocab_size]\n",
        "    \n",
        "    # Get top-20 tokens\n",
        "    top_k = 20\n",
        "    top_probs_original, top_indices = torch.topk(torch.softmax(logits, dim=-1), top_k)\n",
        "    top_tokens = [tokenizer.decode([idx.item()]) for idx in top_indices]\n",
        "    \n",
        "    # Apply different temperatures\n",
        "    temperatures = [0.1, 0.5, 1.0, 2.0]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, temp in enumerate(temperatures):\n",
        "        scaled_logits = logits / temp\n",
        "        probs = torch.softmax(scaled_logits, dim=-1)\n",
        "        top_probs = probs[top_indices]\n",
        "        \n",
        "        axes[idx].bar(range(len(top_probs)), top_probs.cpu().numpy())\n",
        "        axes[idx].set_xticks(range(len(top_tokens)))\n",
        "        axes[idx].set_xticklabels(top_tokens, rotation=45, ha='right', fontsize=8)\n",
        "        axes[idx].set_ylabel('Probability', fontsize=10)\n",
        "        axes[idx].set_title(f'Temperature = {temp}', fontsize=12, fontweight='bold')\n",
        "        axes[idx].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.suptitle('Effect of Temperature on Token Probabilities', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Top-K Sampling\n",
        "\n",
        "Top-k sampling restricts sampling to the k most likely tokens, reducing the chance of generating unlikely tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different top-k values\n",
        "prompt = \"In a far away land\"\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "top_k_values = [None, 10, 50, 100]\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\\n\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for top_k in top_k_values:\n",
        "    output_ids = generate_text(\n",
        "        model,\n",
        "        input_ids,\n",
        "        maximum_new_tokens=20,\n",
        "        temperature=0.8,\n",
        "        top_k_tokens=top_k\n",
        "    )\n",
        "    output_text = tokenizer.decode(output_ids)\n",
        "    k_str = \"All tokens\" if top_k is None else f\"Top-{top_k}\"\n",
        "    print(f\"{k_str:15s}: {output_text}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Greedy Decoding (Temperature = 0)\n",
        "\n",
        "Greedy decoding always picks the most likely token. This is deterministic but can be repetitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Greedy decoding (temperature = 0)\n",
        "prompt = \"The little girl\"\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\\n\")\n",
        "\n",
        "# Greedy\n",
        "output_ids_greedy = generate_text(\n",
        "    model,\n",
        "    input_ids,\n",
        "    maximum_new_tokens=20,\n",
        "    temperature=0.0,  # Greedy\n",
        "    top_k_tokens=None\n",
        ")\n",
        "output_text_greedy = tokenizer.decode(output_ids_greedy)\n",
        "print(f\"Greedy (temp=0): {output_text_greedy}\")\n",
        "\n",
        "# With temperature\n",
        "output_ids_temp = generate_text(\n",
        "    model,\n",
        "    input_ids,\n",
        "    maximum_new_tokens=20,\n",
        "    temperature=0.8,\n",
        "    top_k_tokens=None\n",
        ")\n",
        "output_text_temp = tokenizer.decode(output_ids_temp)\n",
        "print(f\"Sampling (temp=0.8): {output_text_temp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Multiple Generations\n",
        "\n",
        "Generate multiple samples to see the diversity of outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate multiple samples\n",
        "prompt = \"Once upon a time\"\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "num_samples = 5\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\\n\")\n",
        "print(\"Generating 5 samples with temperature=0.8:\\n\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i in range(num_samples):\n",
        "    output_ids = generate_text(\n",
        "        model,\n",
        "        input_ids,\n",
        "        maximum_new_tokens=25,\n",
        "        temperature=0.8,\n",
        "        top_k_tokens=50\n",
        "    )\n",
        "    output_text = tokenizer.decode(output_ids)\n",
        "    print(f\"Sample {i+1}: {output_text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Comparing Generation Strategies\n",
        "\n",
        "Let's compare different combinations of temperature and top-k."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different strategies\n",
        "prompt = \"The sun was shining\"\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "\n",
        "strategies = [\n",
        "    (\"Greedy\", 0.0, None),\n",
        "    (\"Low temp\", 0.3, None),\n",
        "    (\"Medium temp\", 0.8, None),\n",
        "    (\"High temp\", 1.5, None),\n",
        "    (\"Top-k=10\", 0.8, 10),\n",
        "    (\"Top-k=50\", 0.8, 50),\n",
        "    (\"Top-k=100\", 0.8, 100),\n",
        "]\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for name, temp, top_k in strategies:\n",
        "    output_ids = generate_text(\n",
        "        model,\n",
        "        input_ids,\n",
        "        maximum_new_tokens=20,\n",
        "        temperature=temp,\n",
        "        top_k_tokens=top_k\n",
        "    )\n",
        "    output_text = tokenizer.decode(output_ids)\n",
        "    print(f\"{name:15s} (temp={temp}, top_k={top_k}): {output_text}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Understanding Generation Step by Step\n",
        "\n",
        "Let's manually trace through one generation step to understand how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual generation step\n",
        "prompt = \"Hello\"\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "input_tensor = torch.tensor([input_ids], device=device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Forward pass\n",
        "    logits = model(input_tensor)  # [1, seq_len, vocab_size]\n",
        "    \n",
        "    # Get logits for last position\n",
        "    last_logits = logits[0, -1, :]  # [vocab_size]\n",
        "    \n",
        "    # Apply temperature\n",
        "    temperature = 0.8\n",
        "    scaled_logits = last_logits / temperature\n",
        "    \n",
        "    # Apply top-k\n",
        "    top_k = 50\n",
        "    top_k_values, top_k_indices = torch.topk(scaled_logits, min(top_k, len(scaled_logits)))\n",
        "    threshold = top_k_values[-1]\n",
        "    scaled_logits = scaled_logits.masked_fill(scaled_logits < threshold, float('-inf'))\n",
        "    \n",
        "    # Softmax to get probabilities\n",
        "    probs = torch.softmax(scaled_logits, dim=-1)\n",
        "    \n",
        "    # Get top-10 most likely tokens\n",
        "    top_10_probs, top_10_indices = torch.topk(probs, 10)\n",
        "    top_10_tokens = [tokenizer.decode([idx.item()]) for idx in top_10_indices]\n",
        "    \n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(f\"\\nTop 10 most likely next tokens:\")\n",
        "    for i, (prob, token) in enumerate(zip(top_10_probs, top_10_tokens)):\n",
        "        print(f\"  {i+1:2d}. '{token}' (probability: {prob.item():.4f})\")\n",
        "    \n",
        "    # Sample from distribution\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "    next_token_str = tokenizer.decode([next_token.item()])\n",
        "    print(f\"\\nSampled token: '{next_token_str}' (ID: {next_token.item()})\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
